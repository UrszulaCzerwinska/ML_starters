{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as sm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis & vizualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() #get summary of the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentaire :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(figsize=(10,10), rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.VAR.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tps = df.copy()\n",
    "df_tps['time'] = pd.to_datetime(df_tps.dteday)\n",
    "\n",
    "df_tps['time'] +=  pd.to_timedelta(df_tps.hr, unit='h')\n",
    "\n",
    "df_tps.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tps.set_index('time', inplace=True)\n",
    "\n",
    "df_tps.plot(y =['registered','casual'], figsize=(60,20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot scatter plots with correlation coefficient\n",
    "from pandas.plotting import scatter_matrix\n",
    "df_cont_vars = df.select_dtypes(include=['float64', 'int'])\n",
    "scatter_pl = scatter_matrix(df_cont_vars, figsize=(10,10), diagonal='kde') #get scatter plot for all continous variables at once \n",
    "corr = df_cont_vars.corr().as_matrix()\n",
    "for i, j in zip(*plt.np.triu_indices_from(scatter_pl, k=1)):\n",
    "    scatter_pl[i, j].annotate(\"%.3f\" %corr[i,j], (0.8, 0.8), xycoords='axes fraction', ha='center', va='center')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical approach to regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = \"target~\"+\"+\".join(X_train.columns.values)\n",
    "Xy_train_log= X_train.assign(target = y_train_log)\n",
    "ols_reg_log = sm.ols(all_columns,Xy_train_log).fit()#can serve to better understand the regression and feature importance \n",
    "ols_reg = sm.ols(all_columns,Xy_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_reg_log.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_reg_log.summary() # from the summary it looks like X are not usefull features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter( ols_reg.fittedvalues, ols_reg.resid, alpha=0.2)\n",
    "residuals_plot = sns.residplot(ols_reg.fittedvalues, 'target', data=Xy_train, \n",
    "                          lowess=True, \n",
    "                          scatter_kws={'alpha': 0.3}, \n",
    "                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "residuals_plot.set_title(\"Residuals vs. Predicted: non-log\")\n",
    "residuals_plot.set_ylabel('Predicted')\n",
    "residuals_plot.set_xlabel('Residuals')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_plot = sns.residplot(ols_reg_log.fittedvalues, 'target', data=Xy_train, \n",
    "                          lowess=True, \n",
    "                          scatter_kws={'alpha': 0.3}, \n",
    "                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "residuals_plot.set_title(\"Residuals vs. Predicted: log\")\n",
    "residuals_plot.set_ylabel('Predicted')\n",
    "residuals_plot.set_xlabel('Residuals')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "qq_plot = ProbPlot(ols_reg.get_influence().resid_studentized_internal)\n",
    "qq_plot2 = qq_plot.qqplot(line='45', alpha=0.5, color='blue', lw=1)\n",
    "qq_plot2.axes[0].set_title('QQplot: non-log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "QQplot shows how well the distribution of residuals fit the normal distribution. This plots the standardized (z-score) residuals against the theoretical normal quantiles. Anything quite off the diagonal lines may be a concern for further investigation.\n",
    "\n",
    "QQ plots here looks ok, not many pmoints are off diagonal in the log response model, the situation is bad in the non-log model => another reason to log the response varaible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laverage_pl = sns.regplot(ols_reg.get_influence().hat_matrix_diag, ols_reg.get_influence().resid_studentized_internal, \n",
    "            scatter=True, \n",
    "            ci=False, \n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n",
    "\n",
    "laverage_pl.set_title('Residuals vs Leverage: non_log')\n",
    "laverage_pl.set_xlabel('Leverage')\n",
    "laverage_pl.set_ylabel('Standardized Residuals')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "\n",
    "The 'Residuals vs Leverage' allows study the impact of outliers. It looks like there is a lot of outliers that impact the regression that is similar for log and non-log models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define bins for stratification\n",
    "bins = np.linspace(0, df.shape[0], 100)\n",
    "# Save your Y values in a new ndarray,\n",
    "# broken down by the bins created above.\n",
    "y_binned = np.digitize(bikes['cnt'], bins)\n",
    "\n",
    "\n",
    "#split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df#.drop(\n",
    "                                                    #columns = ['instant','dteday','cnt','atemp',\n",
    "                                                    #           'registered', 'casual']), #X\n",
    "                                                    df['target'], #Y\n",
    "                                                    random_state=123, #seed\n",
    "                                                    stratify=y_binned, #have a representative sample\n",
    "                                                    test_size=0.3) #using 20% for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "#from sklearn.svm import SVR\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "regression_models = [('LinearRegression', linear_model.LinearRegression()), #baseline\n",
    "                     ('Lasso', linear_model.Lasso()), #regularization L1\n",
    "                      ('Ridge', linear_model.Ridge()),#regularization L2\n",
    "                   #  ('RANSAC', linear_model.RANSACRegressor()), #to cope with outliers\n",
    "                  #     ('SVR_rbf', SVR(kernel='rbf', C=1e3, gamma=0.1)), #SVR \n",
    "                   #    ('SVR_lin', SVR(kernel='linear', C=1e3)),#SVR \n",
    "                   #    ('SVR_poly', SVR(kernel='poly', C=1e3, degree=2)),#SVR \n",
    "                       ('KNN_distance', neighbors.KNeighborsRegressor(5, weights=\"distance\")), #kKn\n",
    "                       ('CART', tree.DecisionTreeRegressor()), #CART\n",
    "                       ('Random_forest', RandomForestRegressor()), #random forest\n",
    "                       ('GB', GradientBoostingRegressor()), #boosted trees\n",
    "                       ('XBG', XGBRegressor()) #kaggle champion xgb\n",
    "                       ]\n",
    "# evaluate each model in turn\n",
    "names = []\n",
    "dict_method_score = {}\n",
    "scoring = 'MSE'\n",
    "\n",
    "print(\"Method: MSE\")\n",
    "\n",
    "for name, model in regression_models:\n",
    "    model.random_state = 123\n",
    "    model.fit(X_train, y_train_log)\n",
    "    y_pred = model.predict(X_test)\n",
    "    dict_method_score[name]= mean_squared_error(y_test_log, y_pred)\n",
    "    print(\"{:s}: {:.3f}\".format(name, dict_method_score[name]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scalers = [('Standard', preprocessing.StandardScaler()), \n",
    "            ('MinMax', preprocessing.MinMaxScaler()),\n",
    "           ('Robust', preprocessing.RobustScaler())]\n",
    "            \n",
    "# evaluate each model in turn\n",
    "dict_method_score = {}\n",
    "scoring = 'MSE'\n",
    "\n",
    "print(\"Method: MSE\")\n",
    "scaler = preprocessing.StandardScaler()\n",
    "for name, scaler in scalers:\n",
    "    model = RandomForestRegressor(random_state=123)\n",
    "    X_train_sc = scaler.fit_transform(X_train)\n",
    "    model.fit(X_train_sc, y_train_log)\n",
    "    X_test_sc = scaler.transform(X_test)\n",
    "    y_pred = model.predict(X_test_sc)\n",
    "    dict_method_score[name]= mean_squared_error(y_test_log, y_pred)\n",
    "    print(\"{:s}: {:.3f}\".format(name, dict_method_score[name]))\n",
    "\n",
    "model = RandomForestRegressor(random_state=123)\n",
    "model.fit(X_train, y_train_log)\n",
    "y_pred = model.predict(X_test)\n",
    "dict_method_score[\"No\"]= mean_squared_error(y_test_log, y_pred)\n",
    "print(\"{:s}: {:.3f}\".format(\"No\", dict_method_score[\"No\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers_sd2 = df[np.abs(bikes[\"cnt\"]-bikes[\"cnt\"].mean())<=(2*bikes[\"cnt\"].std())] \n",
    "\n",
    "df_no_outliers_sd3 = df[np.abs(bikes[\"cnt\"]-bikes[\"cnt\"].mean())<=(3*bikes[\"cnt\"].std())] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check interactions of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "inter = PolynomialFeatures(interaction_only=True)\n",
    "\n",
    "#train test split \n",
    "\n",
    "X_train_inter = inter.fit_transform(X_train)\n",
    "X_test_inter = inter.fit_transform(X_test)\n",
    "\n",
    "model = RandomForestRegressor(random_state=123)\n",
    "model.fit(X_train_inter, y_train_log)\n",
    "y_pred = model.predict(X_test_inter)\n",
    "\n",
    "mean_squared_error(y_test, y_pred) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(model.feature_importances_, index=inter.get_feature_names(X_train.columns)).sort_values(ascending=False).plot.bar(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change categorical encoding to hot-one encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=[], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = bikes.drop(columns = ['instant','dteday','cnt','atemp',\n",
    "                                                       'registered', 'casual'])\n",
    "X_sc = preprocessing.StandardScaler().fit_transform(X_all)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=X_all.shape[1])\n",
    "principalComponents = pca.fit(X_sc)\n",
    "X_pca = pca.fit_transform(X_sc)\n",
    "cols = [\"PC%d\" % x for x in range(1, (X_all.shape[1])+1)]\n",
    "\n",
    "#print(pca.explained_variance_)\n",
    "#pca = PCA().fit(digits.data)\n",
    "plt.plot(np.cumsum(principalComponents.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame( data= X_pca, columns = cols)\n",
    "\n",
    "finalDf = pd.concat([principalDf, bikes[['cnt']]], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bins for stratification\n",
    "bins = np.linspace(0, 17379, 100)\n",
    "# Save your Y values in a new ndarray,\n",
    "# broken down by the bins created above.\n",
    "y_binned = np.digitize(finalDf['cnt'], bins)\n",
    "\n",
    "\n",
    "\n",
    "#split\n",
    "X_train, X_test, y_train, y_test = train_test_split(finalDf.drop(\n",
    "                                            columns = ['cnt']), #X\n",
    "                                            finalDf['cnt'], #Y\n",
    "                                            random_state=123, #seed\n",
    "                                            stratify=y_binned, #have a representative sample\n",
    "                                            test_size=0.2) #using 20% for test\n",
    "\n",
    "y_train_log, y_test_log = np.log1p(y_train), np.log1p(y_test) #get log\n",
    "\n",
    "\n",
    "rf = RandomForestRegressor(random_state=123)\n",
    "rf.fit(X_train, y_train_log)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "reg_metric(y_test_log, y_pred) \n",
    "rmsle(y_test_log, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "rf = RandomForestRegressor(random_state=123)\n",
    "# Fit the random search model\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=1, random_state=123, n_jobs = -1, scoring='mean_squared_error', refit=True) #low number of iter to go faster today\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_random.predict(X_test)\n",
    "\n",
    "reg_metric(y_test_log, y_pred) \n",
    "rmsle(y_test_log, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use a library that enable to runs huperparameter optimisation and choses the algorithm presented in the last kaggle newsletter. I am not very familiar with a library so I will use it in a simple way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run hpsklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/URSZULA/Documents/DS_ressources/KG/wiki_web/hyperopt-sklearn\") #some path pb tweaking\n",
    "from hpsklearn import HyperoptEstimator, any_regressor, any_preprocessing\n",
    "from hyperopt import tpe\n",
    "\n",
    "\n",
    "# Download the data and split into training and test sets\n",
    "\n",
    "# iris = load_iris()\n",
    "\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# test_size = int(0.2 * len(y))\n",
    "# np.random.seed(13)\n",
    "# indices = np.random.permutation(len(X))\n",
    "# X_train = X[indices[:-test_size]]\n",
    "# y_train = y[indices[:-test_size]]\n",
    "# X_test = X[indices[-test_size:]]\n",
    "# y_test = y[indices[-test_size:]]\n",
    "\n",
    "# Instantiate a HyperoptEstimator with the search space and number of evaluations\n",
    "\n",
    "estim = HyperoptEstimator(classifier=any_regressor('my_reg'),\n",
    "                          preprocessing=any_preprocessing('my_pre'),\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=100,\n",
    "                          trial_timeout=120)\n",
    "\n",
    "\n",
    "estim.fit(X_train, y_train)\n",
    "\n",
    "# Show the results\n",
    "\n",
    "print(estim.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estim.best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyse the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had trouble with acessing the hpsklearn features, I will run the model with sklearn using the parameters and pre-processing chosen by hpsklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline#, make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# default params\n",
    "m = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "             max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
    "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "             min_samples_leaf=11, min_samples_split=2,\n",
    "             min_weight_fraction_leaf=0.0, n_estimators=71, n_jobs=1,\n",
    "             oob_score=False, random_state=2, verbose=False,\n",
    "             warm_start=False)\n",
    "pre = MinMaxScaler(copy=True, feature_range=(-1.0, 1.0))\n",
    "p = Pipeline([('preprocessing',pre ),('learner', m) ])\n",
    "p.fit(X_train, y_train)\n",
    "p.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(feat_imp = p.steps[1][1].feature_importances_, index= X_train.columns).sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualizations of the predicted/true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = sns.regplot(y_test, y_pred, color=\"g\", lowess=True, scatter_kws= {'alpha': 0.1}, line_kws={'color': 'red'})\n",
    "pl.text(900, 800, \"R^2 = 0.946\", horizontalalignment='left', size='large', color='black', weight='bold')\n",
    "pl.set_title('Correlation between predicted and true values in test set of the best model', fontdict={'fontsize':20})\n",
    "pl.set_ylabel('predicted number of XXX', fontdict={'fontsize':15})\n",
    "pl.set_xlabel('true number of XXX', fontdict={'fontsize':15})\n",
    "pl.figure.set_size_inches(18.5, 10.5)\n",
    "#sns.plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "series = Series.from_csv('daily-minimum-temperatures.csv', header=0)\n",
    "plot_acf(series)\n",
    "\n",
    "plot_pacf(series, lags=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(timeseries):\n",
    "    \n",
    "    #Determing rolling statistics\n",
    "    rolmean = pd.rolling_mean(timeseries, window=12)\n",
    "    rolstd = pd.rolling_std(timeseries, window=12)\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print 'Results of Dickey-Fuller Test:'\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print dfoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "410px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
